{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Bagging\nBagging, or Bootstrap Aggregating, is an ensemble learning technique that improves the stability and accuracy of machine learning algorithms. It works by training multiple instances of the same learning algorithm on different subsets of the training data and then aggregating their predictions. Bagging helps reduce variance and combat overfitting.","metadata":{}},{"cell_type":"code","source":"# Bagging (Bootstrap Aggregating) Classification Notebook\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Generate synthetic binary classification data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # 100 samples with 2 features\ny = (X[:, 0] + X[:, 1] > 1).astype(int)  # Class 1 if the sum of the features is greater than 1\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Train the Bagging classifier using Decision Trees\nbase_model = DecisionTreeClassifier(random_state=0)\nmodel = BaggingClassifier(base_estimator=base_model, n_estimators=50, random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict using the model\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\n# Print evaluation results\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)\n\n# Visualize the results\nplt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='red', label='Class 0')\nplt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', label='Class 1')\n\n# Create a mesh grid for decision boundary visualization\nx_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\ny_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.3)\nplt.title('Bagging Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T06:29:02.229100Z","iopub.execute_input":"2024-10-01T06:29:02.229501Z","iopub.status.idle":"2024-10-01T06:29:04.768275Z","shell.execute_reply.started":"2024-10-01T06:29:02.229455Z","shell.execute_reply":"2024-10-01T06:29:04.766817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation of Code Components\n\n1. **Data Generation**: Synthetic binary classification data is created where points are classified based on the sum of two features.\n\n2. **Data Preprocessing**: The dataset is split into training and testing sets.\n\n3. **Model Training**: A Bagging classifier is trained using a Decision Tree as the base estimator. The number of base estimators is set using the `n_estimators` parameter.\n\n4. **Prediction**: Predictions are made on the test set.\n\n5. **Model Evaluation**:\n   - **Accuracy**: The proportion of correctly classified instances.\n   - **Confusion Matrix**: A table used to describe the performance of the classification model.\n   - **Classification Report**: Includes precision, recall, and F1-score for each class.\n\n6. **Visualization**: A scatter plot displays the actual class distribution in the test set. The decision boundary created by the Bagging model is visualized using contour plots.\n\n### Note\n- Bagging is particularly effective with high-variance models like Decision Trees. \n- It reduces variance without increasing bias significantly, making it a robust ensemble method.","metadata":{}}]}