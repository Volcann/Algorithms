{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Random Forest\nRandom Forest is an extension of Bagging where the base estimator is a Decision Tree, and at each split in the tree, a random subset of features is considered. This randomness reduces overfitting and improves generalization compared to a single decision tree.","metadata":{}},{"cell_type":"code","source":"# Random Forest Classification Notebook\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Generate synthetic binary classification data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # 100 samples with 2 features\ny = (X[:, 0] + X[:, 1] > 1).astype(int)  # Class 1 if the sum of the features is greater than 1\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Train the Random Forest classifier\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)  # 100 trees in the forest\nmodel.fit(X_train, y_train)\n\n# Predict using the model\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\n# Print evaluation results\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)\n\n# Visualize the results\nplt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='red', label='Class 0')\nplt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', label='Class 1')\n\n# Create a mesh grid for decision boundary visualization\nx_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\ny_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.3)\nplt.title('Random Forest Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-08T09:12:27.479077Z","iopub.execute_input":"2024-10-08T09:12:27.479628Z","iopub.status.idle":"2024-10-08T09:12:29.341721Z","shell.execute_reply.started":"2024-10-08T09:12:27.479565Z","shell.execute_reply":"2024-10-08T09:12:29.340449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation of Code Components\n\n1. **Data Generation**: Synthetic binary classification data is created where points are classified based on the sum of two features.\n\n2. **Data Preprocessing**: The dataset is split into training and testing sets.\n\n3. **Model Training**: A Random Forest classifier is trained using 100 decision trees (`n_estimators=100`). The `random_state` ensures reproducibility of results.\n\n4. **Prediction**: Predictions are made on the test set.\n\n5. **Model Evaluation**:\n   - **Accuracy**: The proportion of correctly classified instances.\n   - **Confusion Matrix**: A table used to describe the performance of the classification model.\n   - **Classification Report**: Includes precision, recall, and F1-score for each class.\n\n6. **Visualization**: A scatter plot displays the actual class distribution in the test set. The decision boundary created by the Random Forest model is visualized using contour plots.\n\n### Key Features of Random Forest\n- **Ensemble Method**: Combines the output of multiple decision trees to improve robustness.\n- **Feature Randomness**: At each split, only a random subset of features is considered, which helps decorrelate the trees and reduce overfitting.\n- **Out-of-Bag (OOB) Error**: Random Forest can also compute OOB error, which is a method of internal cross-validation using the unused bootstrapped data during training.\n\n### Note\n- Random Forest can be used for both classification and regression tasks.\n- You can control the depth of trees, the minimum number of samples for a split, and the number of trees (`n_estimators`) to tune the performance.","metadata":{}}]}