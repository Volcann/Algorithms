{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# L1 Regression (Lasso)","metadata":{}},{"cell_type":"code","source":"# L1 Regression (Lasso) Notebook\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# Generate synthetic data\nnp.random.seed(0)\nX = 2 - 3 * np.random.rand(100)\ny = X**2 + np.random.randn(100) * 0.5\n\n# Reshape the data\nX = X[:, np.newaxis]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Create polynomial features\npoly = PolynomialFeatures(degree=2)\nX_poly = poly.fit_transform(X_train)\n\n# Train the Lasso regression model\nlasso = Lasso(alpha=0.1)  # You can adjust the alpha for regularization strength\nlasso.fit(X_poly, y_train)\n\n# Predict using the model\nX_test_poly = poly.transform(X_test)\ny_pred = lasso.predict(X_test_poly)\n\n# Visualize the results\nplt.scatter(X_test, y_test, color='red', label='Actual data')\nplt.scatter(X_test, y_pred, color='blue', label='Predicted data')\nplt.title('L1 Regression (Lasso) Results')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n# Calculate and print model performance metrics\nmae = mean_absolute_error(y_test, y_pred)\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Model Performance Metrics:\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"R-squared (R²): {r2}\")\n\n# Print model coefficients and intercept\nprint(\"Model coefficients:\", lasso.coef_)\nprint(\"Model intercept:\", lasso.intercept_)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-01T05:43:03.802125Z","iopub.execute_input":"2024-10-01T05:43:03.802583Z","iopub.status.idle":"2024-10-01T05:43:05.652213Z","shell.execute_reply.started":"2024-10-01T05:43:03.802532Z","shell.execute_reply":"2024-10-01T05:43:05.651064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation of Code Components\n\n1. **Data Generation**: Synthetic data is generated for polynomial regression.\n\n2. **Data Preprocessing**: The dataset is split into training and testing sets, and polynomial features are created.\n\n3. **Lasso Model Training**: A Lasso regression model is trained using the training data, with an adjustable `alpha` parameter controlling the regularization strength.\n\n4. **Prediction**: Predictions are made on the test data.\n\n5. **Visualization**: The actual vs. predicted values are plotted.\n\n6. **Performance Measurement**: The model’s performance is evaluated using MAE, MSE, and R² metrics.\n\n7. **Model Coefficients**: The coefficients and intercept of the Lasso model are printed to analyze feature importance.\n\n### Note\nYou can adjust the `alpha` parameter in the Lasso constructor to see how it affects the model performance and feature selection. A larger alpha will increase regularization, potentially leading to more coefficients being shrunk to zero.","metadata":{}}]}