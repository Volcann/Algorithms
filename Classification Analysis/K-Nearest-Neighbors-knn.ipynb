{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30775,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# K-Nearest Neighbors (KNN)\nK-Nearest Neighbors (KNN) is a simple, non-parametric algorithm used for both classification and regression tasks. In KNN classification, the output class is determined by the majority class among the K nearest neighbors in the feature space.","metadata":{}},{"cell_type":"code","source":"# K-Nearest Neighbors (KNN) Classification Notebook\n\n# Import necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n# Generate synthetic binary classification data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # 100 samples with 2 features\ny = (X[:, 0] + X[:, 1] > 1).astype(int)  # Class 1 if the sum of the features is greater than 1\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Train the KNN classifier\nk = 5  # Number of neighbors\nmodel = KNeighborsClassifier(n_neighbors=k)\nmodel.fit(X_train, y_train)\n\n# Predict using the model\ny_pred = model.predict(X_test)\n\n# Evaluate model performance\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\nclass_report = classification_report(y_test, y_pred)\n\n# Print evaluation results\nprint(\"Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\nprint(\"Classification Report:\\n\", class_report)\n\n# Visualize the results\nplt.scatter(X_test[y_test == 0][:, 0], X_test[y_test == 0][:, 1], color='red', label='Class 0')\nplt.scatter(X_test[y_test == 1][:, 0], X_test[y_test == 1][:, 1], color='blue', label='Class 1')\n\n# Create a mesh grid for decision boundary visualization\nx_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\ny_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\nZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.contourf(xx, yy, Z, alpha=0.3)\nplt.title('KNN Decision Boundary')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-01T06:09:40.536012Z","iopub.execute_input":"2024-10-01T06:09:40.537094Z","iopub.status.idle":"2024-10-01T06:09:42.735736Z","shell.execute_reply.started":"2024-10-01T06:09:40.537035Z","shell.execute_reply":"2024-10-01T06:09:42.734310Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Explanation of Code Components\n\n1. **Data Generation**: Synthetic binary classification data is created where points are classified based on the sum of two features.\n\n2. **Data Preprocessing**: The dataset is split into training and testing sets.\n\n3. **Model Training**: A KNN classifier is trained using the training data, specifying the number of neighbors (K).\n\n4. **Prediction**: Predictions are made on the test set.\n\n5. **Model Evaluation**:\n   - **Accuracy**: The proportion of correctly classified instances.\n   - **Confusion Matrix**: A table used to describe the performance of the classification model.\n   - **Classification Report**: Includes precision, recall, and F1-score for each class.\n\n6. **Visualization**: A scatter plot displays the actual class distribution in the test set. The decision boundary created by the KNN model is visualized using contour plots.\n\n### Note\n- The choice of K (number of neighbors) can significantly impact model performance. It can be determined using cross-validation.\n- KNN can be computationally intensive for large datasets since it requires calculating the distance to all training samples during prediction.","metadata":{}}]}